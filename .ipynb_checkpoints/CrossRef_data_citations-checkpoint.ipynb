{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c717c29e-5871-417a-a220-6a65e8980f60",
   "metadata": {},
   "source": [
    "### Workflow to collect citation information for datasets published by NERC data centres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fca82f-54bf-4646-8496-2c361d3eca6d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "# module to run event data queries\n",
    "import crossRef_fun\n",
    "import os # some file manipulations\n",
    "from math import ceil\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import requests\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1260613a-98f9-4b32-a68d-3d0315966677",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.eventdata.crossref.org/v1/events?mailto=Anonymous&rows=1000&obj-id.prefix=10.5285&from-occurred-date=2023-01-01&until-occurred-date=2023-02-01\n",
      "Event Data query started...\n",
      "API query complete  200\n",
      "output file written to C:/Users/matnic/OneDrive/OneDrive - UKCEH/Projects/DataCentre Citations/Results/event_data_10.5285_2023-01-01_2023-02-01.json\n",
      "543 events found\n",
      "https://api.eventdata.crossref.org/v1/events?mailto=Anonymous&cursor=973ad393-9c04-4dfb-aea3-b525ebd0a92d&rows=1000&obj-id.prefix=10.5285&from-occurred-date=2023-01-01&until-occurred-date=2023-02-01\n",
      "Event Data query started...\n",
      "API query complete  200\n",
      "output file written to C:/Users/matnic/OneDrive/OneDrive - UKCEH/Projects/DataCentre Citations/Results/NERC_EDS_events_from_2023-01-01_up_to_2023-02-01/page0000.json\n"
     ]
    }
   ],
   "source": [
    "# attempting to write as a function\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "from crossRef_fun import getCrossRefCitations\n",
    "email = \"matnic@ceh.ac.uk\"\n",
    "prefix = \"10.5285\"\n",
    "start_date = \"2023-01-01\"\n",
    "end_date = \"2023-02-01\"\n",
    "results_folder_path = \"C:/Users/matnic/OneDrive/OneDrive - UKCEH/Projects/DataCentre Citations/Results/\"\n",
    "results_folder = results_folder_path + \"NERC_EDS_events_from_\" + start_date + \"_up_to_\" + end_date\n",
    "\n",
    "getCrossRefCitations.getCrossRefCitations_byDates(email, prefix, start_date, end_date, results_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee222573-41d0-4bc1-9266-a65163aa0956",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "email = \"matnic@ceh.ac.uk\"\n",
    "prefix = \"10.5285\"\n",
    "start_date = \"2023-01-01\"\n",
    "end_date = \"2023-03-01\" \n",
    "results_folder_path = \"C:/Users/matnic/OneDrive/OneDrive - UKCEH/Projects/DataCentre Citations/Results/\"\n",
    "# something to check date is valid\n",
    "try:\n",
    "    datetime.date.fromisoformat(start_date)\n",
    "    datetime.date.fromisoformat(end_date)\n",
    "except Exception as e:\n",
    "    print(\"start_date and/or end_date in wrong format. Should be yyyy-mm-dd\")\n",
    "\n",
    "# filename to save json event data to\n",
    "filename = results_folder_path + \"crossRef_event_data_\" + prefix + \"_\" + start_date + \"_\" + end_date + \".json\"\n",
    "\n",
    "# Set up the query\n",
    "ed = crossRef_fun.eventData(email = email, outputFile = filename)\n",
    "ed.buildQuery({'obj-id.prefix' : prefix, 'from-occurred-date' : start_date, 'until-occurred-date' : end_date}) \n",
    "\n",
    "# run the query to determine number of events\n",
    "ed.runQuery(retry = 5) # scholix = False - can query scholix api as well - worth exploring\n",
    "\n",
    "# calculate how many pages will need to be iterated over\n",
    "num_pages = ceil(ed.events.count()/1000)\n",
    "\n",
    "# set up folder to result jsons into\n",
    "results_folder = results_folder_path + \"crossRef_NERC_EDS_events_from_\" + start_date + \"_up_to_\" + end_date\n",
    "os.mkdir(results_folder) # not able to overwrite folder of the same name - delete folder and re-write?, or, add a folder with a new name each time?\n",
    "\n",
    "# find info from all the pages\n",
    "ed.getAllPages(num_pages, {'rows': 1000, 'obj-id.prefix' : prefix, 'from-occurred-date' : start_date, 'until-occurred-date' : end_date}, fileprefix = (results_folder + '/page')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b54a5bb-07e4-4c9f-b804-6a147214d7a4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code for getting results based on a list of DOIs rather than a date range\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import mrced2 # module to run event data queries\n",
    "from getDataCiteInfo import getDataCiteInfo\n",
    "import os # some file manipulations\n",
    "from math import ceil\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import requests\n",
    "import datetime\n",
    "\n",
    "email = \"matnic@ceh.ac.uk\"\n",
    "\n",
    "# import list of DOIs to find event data for\n",
    "doi_list = pd.read_csv(\"marika_doi_list.csv\", header = None)\n",
    "\n",
    "# set up folder to result jsons into\n",
    "results_folder_path = \"C:/Users/matnic/OneDrive/OneDrive - UKCEH/Projects/DataCentre Citations/Results/\"\n",
    "results_folder_name = \"NERC_EDS_events_from_doi_list_scholix\"\n",
    "results_folder = results_folder_path + results_folder\n",
    "\n",
    "os.makedirs(results_folder, exist_ok=True)\n",
    "\n",
    "for count, doi in enumerate(doi_list[0]):\n",
    "    print(doi)\n",
    "\n",
    "    fileDoi = doi.replace(\"/\", \"_\")\n",
    "    \n",
    "    # Set up the query\n",
    "    # filename to save json event data to\n",
    "    filename = f\"{results_folder}/event_data_{fileDoi}_{count}.json\"\n",
    "    ed = mrced2.eventData(email=email, outputFile=filename)\n",
    "    ed.buildQuery({'obj-id': doi})\n",
    "    \n",
    "    # run the query to determine number of events\n",
    "    ed.runQuery(retry=5) # scholix = False - can query scholix api as well - worth exploring\n",
    "    \n",
    "    # calculate how many pages will need to be iterated over\n",
    "    num_pages = ceil(ed.events.count()/1000)\n",
    "    \n",
    "    # find info from all the pages\n",
    "    ed.getAllPages(num_pages, {'rows': 1000, 'obj-id': doi}, fileprefix=f\"{results_folder}/{fileDoi}_page{count}\") # fileDoi to give each result json a unique name, otherwise it writes over previous results \n",
    "\n",
    "print('Done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3ef04b-7ba2-4cbb-a151-b0baf90d933c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# instance of a class to interpret the events\n",
    "jd1 = crossRef_fun.eventRecord()\n",
    "\n",
    "# get all the filenames\n",
    "files = os.listdir(results_folder)\n",
    "\n",
    "# load the json event data from multiple files\n",
    "jd1.mergeJsons(files, folder = results_folder)\n",
    "\n",
    "## filter out twitter wikipedia etc - later add options to include these info\n",
    "filters = {\"source_id\" : ['twitter', 'wikipedia', 'newsfeed', 'wordpressdotcom', 'reddit-links']}\n",
    "filtered_info = jd1.filter(filters, mode = 'NOT')\n",
    "\n",
    "# collect relevant citation info for NERC project\n",
    "citationInfo = filtered_info.collectCitationInfo()\n",
    "\n",
    "# convert to dataframe\n",
    "crossRef_df = pd.DataFrame(citationInfo)\n",
    "\n",
    "# filter out gbif registrant code prefix 10.15468\n",
    "crossRef_df_gbif_filtered = crossRef_df[~crossRef_df.subj_id.str.contains(\"10.15468\")]\n",
    "\n",
    "# filter out relationship_type_id values that we don't want - is_referenced_by, discusses\n",
    "crossRef_df_gbif_filtered2 = crossRef_df_gbif_filtered[~crossRef_df_gbif_filtered.relation_type_id.str.contains(\"is_referenced_by|discusses|is_new_version_of|is_supplemented_by|is_previous_version_of\")]\n",
    "\n",
    "# remove duplicate subj_ids for each obj_id - e.g. 10.5285/a7f28dea-64f7-43b5-bc39-a6cfcdeefbda has multiple references from 10.5285/65140444-b5fa-4a5e-9ab4-e86c106051e2\n",
    "# find rows where obj_id and subj_id are the same - should I match any other columns?\n",
    "dups = crossRef_df_gbif_filtered2.duplicated(subset=['obj_id', 'subj_id'])\n",
    "crossRef_df_gbif_filtered2_deduplicated = crossRef_df_gbif_filtered2.drop(crossRef_df_gbif_filtered2[dups].index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb05f32b-132d-4438-8231-8923dc7df568",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pass citation info to datacite API to collect relevant info on the datasets, data centres etc\n",
    "(errors, dataCite_df) = getDataCiteInfo(crossRef_df_gbif_filtered2_deduplicated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f25bdce-328c-4fbd-a8bf-b86b404943ba",
   "metadata": {},
   "source": [
    "Pass citation info to datacite API to collect relevant info on the datasets, data centres etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3980f3fb-f698-4526-bf38-b5baade0bb1b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "errors\n",
    "dataCite_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83818d3b-ac2f-4bb2-a510-4f0054b33a9e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # What to do with publication type information? if posted content probably want to ignore?\n",
    "# crossRef_df_gbif_filtered2_deduplicated['obj_id']\n",
    "\n",
    "# # for each dataset count each type of publication that cites it\n",
    "# for data_doi in crossRef_df_processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b631b3-5ef4-493b-8a03-46ccaab2f5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge crossRef_df and dataCite_df to get dataset info in crossref_df\n",
    "# for each column create a mapping pair of dataset DOI and that column name, but skips first column 'Dataset_DOI' in loop\n",
    "for ii in dataCite_df.columns[1:]: # \n",
    "    \n",
    "    # create dictionary of data_doi, value pairs \n",
    "    d = dataCite_df.set_index('dataset_DOI')[ii].to_dict()\n",
    "    \n",
    "    # use the data doi to map the dictionary to the crossref_df\n",
    "    crossRef_df_gbif_filtered2_deduplicated.loc[:,ii] = crossRef_df_gbif_filtered2_deduplicated.obj_id.map(d)\n",
    "\n",
    "    \n",
    "# create data frame that just lists each dataset and has citations counts from crossref, scholex, datacite etc\n",
    "# remove rows from crossref with duplicated obj_id\n",
    "dups = crossRef_df_gbif_filtered2_deduplicated.duplicated('obj_id')\n",
    "dataset_df = crossRef_df_gbif_filtered2_deduplicated.drop(crossRef_df_gbif_filtered2_deduplicated[dups].index)\n",
    "dataset_df = dataset_df.drop(['relation_type_id', 'source_id', 'subj_id', 'subj_work_type_id'], axis = 1) # 'subj_work_type_id'\n",
    "\n",
    "# count how many times each dataset DOI appears in crossRef_df_processed and add this number to dataset_df\n",
    "crossRef_citation_counts = crossRef_df_gbif_filtered2_deduplicated['obj_id'].value_counts()\n",
    "\n",
    "# need counts that include or exclude different relation_type_ids and subj_work_type_id\n",
    "\n",
    "# create dictionary of data_doi, crossRef_citation_counts \n",
    "d = crossRef_citation_counts.to_dict()\n",
    "\n",
    "# use the data doi to map the dictionary to the dataset_df\n",
    "dataset_df['crossRef_citation_count'] = dataset_df.obj_id.map(d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28243c47-ded2-43b7-a0ef-88b915c2fcd1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # pass publication DOIs to DOI.org to determine type of publication using checkDOIpubType function defined above\n",
    "# pubTypeList = []\n",
    "# pubDOI = []\n",
    "\n",
    "# for count, doi in enumerate(crossRef_df_processed['subj_id']):\n",
    "#     doiComponents = doi.split('/')[-2:]\n",
    "#     doi = doiComponents[0] + \"/\" + doiComponents[1]\n",
    "#     # print(type(doi))\n",
    "#     pubType = checkDOIpubType(doi)\n",
    "#     pubTypeList.append(pubType)\n",
    "#     pubDOI.append(doi)\n",
    "    \n",
    "#     # add code to catch retries limit exceeded - might need to be in function itself?\n",
    "#     # e.g. https://stackoverflow.com/questions/23013220/max-retries-exceeded-with-url-in-requests\n",
    "    \n",
    "#     time.sleep(0.5)\n",
    "#     # if count % 200 == 0: # if count is a multiple of 200 wait for a bit\n",
    "#     #         time.sleep(180)\n",
    "\n",
    "# print('Done!')\n",
    "\n",
    "# crossRef_df_processed['publicationType'] = pubTypeList\n",
    "# crossRef_df_processed['publication_DOI'] = pubDOI\n",
    "\n",
    "\n",
    "# # split publicationType column into publicationType1 and publicationSubType columns\n",
    "# crossRef_df_processed[['publicationType1','publicationSubType']] = pd.DataFrame(crossRef_df_processed['publicationType'].tolist(), index=crossRef_df_processed.index)\n",
    "\n",
    "\n",
    "# # determine publication type for records where DOI.org API call failed - ~10 mins\n",
    "# newPubTypeList = []\n",
    "\n",
    "# # get the unknown rows from scholex_df pubtype column and the pubDOI\n",
    "# for pubType, pubDOI in zip(crossRef_df_processed['publicationType1'],crossRef_df_processed['publication_DOI']):\n",
    "    \n",
    "#     if pubType == 'unknown':\n",
    "\n",
    "#         # need a catcher to make sure pubDOI is a single ID - in the cases where there was no DOI there is more than one ID recorded\n",
    "#         if len(pubDOI) < 100: # if the length is less than 100 (arbitrarily) then it will be a single DOI\n",
    "#             pass\n",
    "#         else: # if there is no DOI skip this record\n",
    "#             newPubTypeList.append(pubType) # leave it the same\n",
    "#             #newPubType = pubType \n",
    "#             continue\n",
    "        \n",
    "#         # determine if crossref or datacite supplies the DOI\n",
    "#         print('Pub DOI: ', pubDOI)\n",
    "#         r = requests.get(('https://doi.org/doiRA/' + pubDOI), headers={\"Accept\": \"application/json\"})\n",
    "#         DOIregistry = r.json()[0]['RA']\n",
    "#         print(DOIregistry)\n",
    "\n",
    "#         # query the crossref or datacite API\n",
    "#         if DOIregistry == 'DataCite':\n",
    "#             # ask the Datacite API what type of publication\n",
    "#             r = requests.get(('https://api.datacite.org/dois/' + pubDOI), headers = {'client-id': 'bl.nerc'})\n",
    "#             print(r.status_code)\n",
    "#             newPubTypeList.append(r.json()['data']['attributes']['types']['citeproc']) # is citeproc the correct one to use?\n",
    "\n",
    "#         elif DOIregistry == 'Crossref':\n",
    "#             r = requests.get(('https://api.crossref.org/works/'  + pubDOI), headers={\"Accept\": \"application/json\"})\n",
    "#             print(r.status_code)\n",
    "#             newPubTypeList.append(r.json()['message']['type']) # could also add 'subtype':r.json()['message']['subtype']\n",
    "#         else:\n",
    "#             print('Unknown DOI registry')\n",
    "        \n",
    "#     else: \n",
    "#         newPubTypeList.append(pubType) # in the cases where the pubType is not unknown keep it the same\n",
    "\n",
    "# crossRef_df_processed['newPubTypeList'] = newPubTypeList\n",
    "# print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672adcfb-6744-4f4d-a48e-66188c74c314",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pubDOI = '10.1007/s11368-018-1990-7'\n",
    "r = requests.get(('https://doi.org/doiRA/' + pubDOI), headers={\"Accept\": \"application/json\"})\n",
    "DOIregistry = r.json()[0]['RA']\n",
    "print(DOIregistry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45b7ae7-2ff0-48e1-b4db-301fa9a8fd85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# query the crossref or datacite API\n",
    "if DOIregistry == 'DataCite':\n",
    "    # ask the Datacite API what type of publication\n",
    "    r = requests.get(('https://api.datacite.org/dois/' + pubDOI), headers = {'client-id': 'bl.nerc'})\n",
    "    print(r.status_code)\n",
    "    newPubTypeList.append(r.json()['data']['attributes']['types']['citeproc']) # is citeproc the correct one to use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20bd688-46aa-45fc-836d-48af4247e805",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "r = requests.get(('https://api.crossref.org/works/'  + pubDOI), headers={\"Accept\": \"application/json\"})\n",
    "print(r.status_code)\n",
    "r.json()['message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7903cf-5ea8-41e4-9d07-ca5cfd878442",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# add a section to get publication info from DOI.org api to add to crossRef_df_gbif_filtered2_deduplicated? e.g. title, authors etc\n",
    "doi_url = 'https://doi.org/10.5194/gmd-11-1377-2018'\n",
    "print(doi_url)\n",
    "r = requests.get(doi_url, headers={\"Accept\": \"application/json\"}) # sometimes throws up unexpected errors\n",
    "\n",
    "# sometimes the API call returns info in a format that is not JSON \n",
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79772005-60da-4999-abad-f18ecfb087e1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# very slow if crossRef_df_gbif_filtered2_deduplicated is large - about 1 hour for everything from year 2000\n",
    "# CHANGE THIS TO CROSSREF API BECAUSE DOI.ORG API IS bad\n",
    "# section to get publication info from DOI.org api to add to crossRef_df_gbif_filtered2_deduplicated? e.g. title, authors etc\n",
    "pub_info = []\n",
    "for pubdoi in crossRef_df_gbif_filtered2_deduplicated['subj_id']:\n",
    "    r = requests.get(pubdoi, headers={\"Accept\": \"application/json\"})\n",
    "    \n",
    "    try:\n",
    "        title = r.json()['title']\n",
    "    except:\n",
    "        title = \"Info not given\"\n",
    "    \n",
    "    try:           \n",
    "        authors = []\n",
    "        for jj in range(len(r.json()['author'])):\n",
    "            authors.append([r.json()['author'][jj]['given'],r.json()['author'][jj]['family']])\n",
    "    except:\n",
    "        authors = \"Info not given\"\n",
    "    \n",
    "    try:\n",
    "        publisher = r.json()['publisher']\n",
    "    except:\n",
    "        publisher = \"Info not given\"\n",
    "        \n",
    "    pub_info.append({\n",
    "            'pub_doi': pubdoi,\n",
    "            'pub_Title': title,\n",
    "            'pub_authors': authors,\n",
    "            'publisher': publisher\n",
    "        # add publication date to this - in order to check if this is before the dataset publication date - could be a way to filter out dodgy results\n",
    "    })\n",
    "    \n",
    "    print(pubdoi)\n",
    "    \n",
    "# add new publication info columns to dataframe\n",
    "pubInfo_df = pd.DataFrame(pub_info)\n",
    "\n",
    "# loop through new columns to be added to df\n",
    "for ii in pubInfo_df.columns[1:]:\n",
    "    # create dictionary of doi, value pairs \n",
    "    d = pubInfo_df.set_index('pub_doi')[ii].to_dict()\n",
    "    \n",
    "    # use the doi to map the dictionary to crossRef_df_gbif_filtered2_deduplicated\n",
    "    crossRef_df_gbif_filtered2_deduplicated.loc[:,ii] = crossRef_df_gbif_filtered2_deduplicated.subj_id.map(d)\n",
    "    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b844a8-502a-41db-ae60-9df820b60d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a step to remove results not from an approved list of publishers/journals?\n",
    "\n",
    "# how to make a list of approved publishers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897159cb-00e4-4274-8672-a23e9b3e92e4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "crossRef_df_gbif_filtered2_deduplicated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f081c8eb-97f7-4465-924c-36ad1618508e",
   "metadata": {},
   "source": [
    "### Get the citation string (APA format) of the publication that has cited the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f13117-8606-4081-9248-3810075e811b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAKES A LONG TIME - can skip this if you don't want this info\n",
    "citationStrList = [] # create an empty list in which to put the citation strings\n",
    "\n",
    "for pubDOI in crossRef_df_gbif_filtered2_deduplicated['subj_id']:\n",
    "    \n",
    "    r = requests.get((pubDOI), headers={\"Accept\": \"text/x-bibliography\", \"style\": \"apa\"})\n",
    "    #print(r.status_code)\n",
    "    citationStrList.append(r.text) # add the citation strings to the list\n",
    "    \n",
    "crossRef_df_gbif_filtered2_deduplicated['PubCitationStr'] = citationStrList # add the citation string list to the Scholex df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dc58a3-bcd7-4260-a18f-147555633bd8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "crossRef_df_gbif_filtered2_deduplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e8ba6a-76e0-4781-87d4-7e42bc6579dd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6a10d2-0567-4bbd-8e45-4039c8270d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add section comparing counts from crossref, DataCite and scholex\n",
    "# first build a tool that takes one dataset and compares citation results from each source\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b204b98-10a2-4608-9cb7-324ed01c3a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output csv file\n",
    "\n",
    "from datetime import date\n",
    "today = date.today()\n",
    "\n",
    "crossRef_df_processed_filename = 'dataset_citation_publication_info_' + start_date + \"_to_\" + end_date + \"_retrieved_\" + (today.strftime(\"%d%m%Y\")) + '.csv'\n",
    "crossRef_df_gbif_filtered2_deduplicated.to_csv(crossRef_df_processed_filename, index = False)\n",
    "print(crossRef_df_processed_filename)\n",
    "\n",
    "dataset_filename = 'dataset_citation_counts_' + start_date + \"_to_\" + end_date + \"_retrieved_\" + (today.strftime(\"%d%m%Y\")) + '.csv'\n",
    "dataset_df.to_csv(dataset_filename, index = False)\n",
    "print(dataset_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95f8f83-04cb-42d8-818a-725ed8b53d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output csv file if based on list of DOIs rather than dates\n",
    "\n",
    "from datetime import date\n",
    "today = date.today()\n",
    "doi_list_name = \"marika_doi_list_scholix\"\n",
    "\n",
    "crossRef_df_processed_filename = 'dataset_citation_publication_info_' + doi_list_name + \"_retrieved_\" + (today.strftime(\"%d%m%Y\")) + '.csv'\n",
    "crossRef_df_gbif_filtered2_deduplicated.to_csv(crossRef_df_processed_filename, index = False)\n",
    "print(crossRef_df_processed_filename)\n",
    "\n",
    "dataset_filename = 'dataset_citation_counts_' + doi_list_name + \"_retrieved_\" + (today.strftime(\"%d%m%Y\")) + '.csv'\n",
    "dataset_df.to_csv(dataset_filename, index = False)\n",
    "print(dataset_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55242c0-66a2-4b09-b009-eeeaf274b112",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
